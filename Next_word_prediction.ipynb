{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing the required libraries:\n"
      ],
      "metadata": {
        "id": "EIEbrBttxTNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "metadata": {
        "id": "4_Q0VdLFxL0_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = []\n",
        "\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "    \n",
        "print(\"The First Line: \", lines[0])\n",
        "print(\"The Last Line: \", lines[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsENJD2qxjrs",
        "outputId": "d1cd9a16-9967-40c4-93ab-4660b5458ccb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The First Line:  ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "\n",
            "The Last Line:  first to get up and stretch out her young body.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cleaning the data"
      ],
      "metadata": {
        "id": "uGQCsehHx6Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\n",
        "\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "    \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:360]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "yMNYJ4ILxt8K",
        "outputId": "eed3f830-3054-439b-d6ef-a719960248a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
        "new_data = data.translate(translator)\n",
        "\n",
        "new_data[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "bOdi2jW_x5C9",
        "outputId": "a99e6064-d5a5-4408-ef44-9eeb89d517b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = []\n",
        "\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "        \n",
        "data = ' '.join(z)\n",
        "data[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "oxoSHlfLyFid",
        "outputId": "860eca84-d6d0-4e53-a16c-4829e92d02c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization"
      ],
      "metadata": {
        "id": "aNCRpMn6yJdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnWIwM1jyHxW",
        "outputId": "e96c74e4-c795-4422-e36d-51a9e11f903d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICg-zw4gyNLh",
        "outputId": "c581c47c-ca0c-4eb1-b81e-12ce46c92e3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlAfoRxkyO6Y",
        "outputId": "28a6f9f6-7bdc-40fc-a20e-e8b065eeffd6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  3889\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 17,  53],\n",
              "       [ 53, 293],\n",
              "       [293,   2],\n",
              "       [  2,  18],\n",
              "       [ 18, 729],\n",
              "       [729, 135],\n",
              "       [135, 730],\n",
              "       [730, 294],\n",
              "       [294,   8],\n",
              "       [  8, 731]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "H10kWEvIyQ9g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The Data is: \", X[:5])\n",
        "print(\"The responses are: \", y[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBg7qOJKyTIQ",
        "outputId": "3fdf2f1c-3575-4dfa-b352-d3385c3d0109"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Data is:  [ 17  53 293   2  18]\n",
            "The responses are:  [ 53 293   2  18 729]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJifAJ0uyU83",
        "outputId": "3b5f8c76-5931-455f-ce16-120b02995a1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#creating the model"
      ],
      "metadata": {
        "id": "Tm3CNNtXyY4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "gGfSO-kayW2M"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yBs0hPJyec_",
        "outputId": "304051ce-1204-4889-e93d-a35be41b1581"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             26170     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2617)              2619617   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,694,787\n",
            "Trainable params: 15,694,787\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot the Model:"
      ],
      "metadata": {
        "id": "4sjqIi34yivn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "7iiywit_yhTd",
        "outputId": "bd36da77-938f-425f-e0c9-601360531e7f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Callbacks:"
      ],
      "metadata": {
        "id": "HfapMfA7yphU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ],
      "metadata": {
        "id": "7TA2HcPTynDL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compile the Model"
      ],
      "metadata": {
        "id": "6r9bRcFwywg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBmLIDrdytS6",
        "outputId": "7f3fbae6-31c9-4602-a2d4-ad3db2459988"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fit the Model"
      ],
      "metadata": {
        "id": "0KjEOi-ozFqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ81sY0zy0m0",
        "outputId": "cff86756-0c81-4885-ba0d-795a46537ab8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8756\n",
            "Epoch 1: loss improved from inf to 7.87555, saving model to nextword1.h5\n",
            "61/61 [==============================] - 22s 267ms/step - loss: 7.8756 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8640\n",
            "Epoch 2: loss improved from 7.87555 to 7.86402, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 249ms/step - loss: 7.8640 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8196\n",
            "Epoch 3: loss improved from 7.86402 to 7.81963, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 7.8196 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.6450\n",
            "Epoch 4: loss improved from 7.81963 to 7.64497, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 263ms/step - loss: 7.6450 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.4241\n",
            "Epoch 5: loss improved from 7.64497 to 7.42407, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 7.4241 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.2568\n",
            "Epoch 6: loss improved from 7.42407 to 7.25684, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 7.2568 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.1519\n",
            "Epoch 7: loss improved from 7.25684 to 7.15193, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 249ms/step - loss: 7.1519 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.0727\n",
            "Epoch 8: loss improved from 7.15193 to 7.07268, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 246ms/step - loss: 7.0727 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.0011\n",
            "Epoch 9: loss improved from 7.07268 to 7.00111, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 251ms/step - loss: 7.0011 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.9133\n",
            "Epoch 10: loss improved from 7.00111 to 6.91326, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 246ms/step - loss: 6.9133 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.8025\n",
            "Epoch 11: loss improved from 6.91326 to 6.80253, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 6.8025 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.6563\n",
            "Epoch 12: loss improved from 6.80253 to 6.65634, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 6.6563 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.4557\n",
            "Epoch 13: loss improved from 6.65634 to 6.45567, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 297ms/step - loss: 6.4557 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.2444\n",
            "Epoch 14: loss improved from 6.45567 to 6.24437, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 262ms/step - loss: 6.2444 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.0566\n",
            "Epoch 15: loss improved from 6.24437 to 6.05658, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 6.0566 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.8743\n",
            "Epoch 16: loss improved from 6.05658 to 5.87433, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 257ms/step - loss: 5.8743 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.6854\n",
            "Epoch 17: loss improved from 5.87433 to 5.68540, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 269ms/step - loss: 5.6854 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.5305\n",
            "Epoch 18: loss improved from 5.68540 to 5.53053, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 255ms/step - loss: 5.5305 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.3698\n",
            "Epoch 19: loss improved from 5.53053 to 5.36982, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 268ms/step - loss: 5.3698 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.2318\n",
            "Epoch 20: loss improved from 5.36982 to 5.23176, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 254ms/step - loss: 5.2318 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.1077\n",
            "Epoch 21: loss improved from 5.23176 to 5.10772, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 257ms/step - loss: 5.1077 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.0086\n",
            "Epoch 22: loss improved from 5.10772 to 5.00856, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 259ms/step - loss: 5.0086 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.9061\n",
            "Epoch 23: loss improved from 5.00856 to 4.90614, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 267ms/step - loss: 4.9061 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.8051\n",
            "Epoch 24: loss improved from 4.90614 to 4.80515, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 257ms/step - loss: 4.8051 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.7375\n",
            "Epoch 25: loss improved from 4.80515 to 4.73754, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 272ms/step - loss: 4.7375 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6381\n",
            "Epoch 26: loss improved from 4.73754 to 4.63809, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 258ms/step - loss: 4.6381 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.5571\n",
            "Epoch 27: loss improved from 4.63809 to 4.55713, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 4.5571 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4753\n",
            "Epoch 28: loss improved from 4.55713 to 4.47533, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 252ms/step - loss: 4.4753 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.4274\n",
            "Epoch 29: loss improved from 4.47533 to 4.42740, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 248ms/step - loss: 4.4274 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3689\n",
            "Epoch 30: loss improved from 4.42740 to 4.36893, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 256ms/step - loss: 4.3689 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3051\n",
            "Epoch 31: loss improved from 4.36893 to 4.30506, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 250ms/step - loss: 4.3051 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.2433\n",
            "Epoch 32: loss improved from 4.30506 to 4.24327, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 262ms/step - loss: 4.2433 - lr: 0.0010\n",
            "Epoch 33/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.1542\n",
            "Epoch 33: loss improved from 4.24327 to 4.15425, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 255ms/step - loss: 4.1542 - lr: 0.0010\n",
            "Epoch 34/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0888\n",
            "Epoch 34: loss improved from 4.15425 to 4.08883, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 256ms/step - loss: 4.0888 - lr: 0.0010\n",
            "Epoch 35/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0104\n",
            "Epoch 35: loss improved from 4.08883 to 4.01035, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 272ms/step - loss: 4.0104 - lr: 0.0010\n",
            "Epoch 36/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9461\n",
            "Epoch 36: loss improved from 4.01035 to 3.94606, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 3.9461 - lr: 0.0010\n",
            "Epoch 37/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.9037\n",
            "Epoch 37: loss improved from 3.94606 to 3.90375, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 240ms/step - loss: 3.9037 - lr: 0.0010\n",
            "Epoch 38/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.8148\n",
            "Epoch 38: loss improved from 3.90375 to 3.81476, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 252ms/step - loss: 3.8148 - lr: 0.0010\n",
            "Epoch 39/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7383\n",
            "Epoch 39: loss improved from 3.81476 to 3.73826, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 239ms/step - loss: 3.7383 - lr: 0.0010\n",
            "Epoch 40/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.6300\n",
            "Epoch 40: loss improved from 3.73826 to 3.62996, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 260ms/step - loss: 3.6300 - lr: 0.0010\n",
            "Epoch 41/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.5269\n",
            "Epoch 41: loss improved from 3.62996 to 3.52693, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 250ms/step - loss: 3.5269 - lr: 0.0010\n",
            "Epoch 42/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.4399\n",
            "Epoch 42: loss improved from 3.52693 to 3.43987, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 3.4399 - lr: 0.0010\n",
            "Epoch 43/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.3576\n",
            "Epoch 43: loss improved from 3.43987 to 3.35757, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 259ms/step - loss: 3.3576 - lr: 0.0010\n",
            "Epoch 44/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.2913\n",
            "Epoch 44: loss improved from 3.35757 to 3.29131, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 253ms/step - loss: 3.2913 - lr: 0.0010\n",
            "Epoch 45/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.1661\n",
            "Epoch 45: loss improved from 3.29131 to 3.16609, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 252ms/step - loss: 3.1661 - lr: 0.0010\n",
            "Epoch 46/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.0811\n",
            "Epoch 46: loss improved from 3.16609 to 3.08112, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 264ms/step - loss: 3.0811 - lr: 0.0010\n",
            "Epoch 47/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.0376\n",
            "Epoch 47: loss improved from 3.08112 to 3.03759, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 3.0376 - lr: 0.0010\n",
            "Epoch 48/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.9577\n",
            "Epoch 48: loss improved from 3.03759 to 2.95772, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 251ms/step - loss: 2.9577 - lr: 0.0010\n",
            "Epoch 49/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8594\n",
            "Epoch 49: loss improved from 2.95772 to 2.85940, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 241ms/step - loss: 2.8594 - lr: 0.0010\n",
            "Epoch 50/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7766\n",
            "Epoch 50: loss improved from 2.85940 to 2.77663, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 245ms/step - loss: 2.7766 - lr: 0.0010\n",
            "Epoch 51/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7028\n",
            "Epoch 51: loss improved from 2.77663 to 2.70282, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 253ms/step - loss: 2.7028 - lr: 0.0010\n",
            "Epoch 52/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6228\n",
            "Epoch 52: loss improved from 2.70282 to 2.62277, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 287ms/step - loss: 2.6228 - lr: 0.0010\n",
            "Epoch 53/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5810\n",
            "Epoch 53: loss improved from 2.62277 to 2.58095, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 249ms/step - loss: 2.5810 - lr: 0.0010\n",
            "Epoch 54/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5404\n",
            "Epoch 54: loss improved from 2.58095 to 2.54036, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 268ms/step - loss: 2.5404 - lr: 0.0010\n",
            "Epoch 55/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4520\n",
            "Epoch 55: loss improved from 2.54036 to 2.45199, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 253ms/step - loss: 2.4520 - lr: 0.0010\n",
            "Epoch 56/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3744\n",
            "Epoch 56: loss improved from 2.45199 to 2.37440, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 261ms/step - loss: 2.3744 - lr: 0.0010\n",
            "Epoch 57/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3328\n",
            "Epoch 57: loss improved from 2.37440 to 2.33280, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 263ms/step - loss: 2.3328 - lr: 0.0010\n",
            "Epoch 58/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2944\n",
            "Epoch 58: loss improved from 2.33280 to 2.29443, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 253ms/step - loss: 2.2944 - lr: 0.0010\n",
            "Epoch 59/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2448\n",
            "Epoch 59: loss improved from 2.29443 to 2.24476, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 2.2448 - lr: 0.0010\n",
            "Epoch 60/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2153\n",
            "Epoch 60: loss improved from 2.24476 to 2.21525, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 271ms/step - loss: 2.2153 - lr: 0.0010\n",
            "Epoch 61/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1652\n",
            "Epoch 61: loss improved from 2.21525 to 2.16523, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 253ms/step - loss: 2.1652 - lr: 0.0010\n",
            "Epoch 62/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1212\n",
            "Epoch 62: loss improved from 2.16523 to 2.12116, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 251ms/step - loss: 2.1212 - lr: 0.0010\n",
            "Epoch 63/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0802\n",
            "Epoch 63: loss improved from 2.12116 to 2.08023, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 268ms/step - loss: 2.0802 - lr: 0.0010\n",
            "Epoch 64/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0456\n",
            "Epoch 64: loss improved from 2.08023 to 2.04562, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 252ms/step - loss: 2.0456 - lr: 0.0010\n",
            "Epoch 65/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0097\n",
            "Epoch 65: loss improved from 2.04562 to 2.00972, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 252ms/step - loss: 2.0097 - lr: 0.0010\n",
            "Epoch 66/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9570\n",
            "Epoch 66: loss improved from 2.00972 to 1.95705, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 250ms/step - loss: 1.9570 - lr: 0.0010\n",
            "Epoch 67/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8956\n",
            "Epoch 67: loss improved from 1.95705 to 1.89559, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 267ms/step - loss: 1.8956 - lr: 0.0010\n",
            "Epoch 68/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8879\n",
            "Epoch 68: loss improved from 1.89559 to 1.88792, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 1.8879 - lr: 0.0010\n",
            "Epoch 69/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8552\n",
            "Epoch 69: loss improved from 1.88792 to 1.85518, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 268ms/step - loss: 1.8552 - lr: 0.0010\n",
            "Epoch 70/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8277\n",
            "Epoch 70: loss improved from 1.85518 to 1.82766, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 263ms/step - loss: 1.8277 - lr: 0.0010\n",
            "Epoch 71/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8126\n",
            "Epoch 71: loss improved from 1.82766 to 1.81262, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 261ms/step - loss: 1.8126 - lr: 0.0010\n",
            "Epoch 72/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7399\n",
            "Epoch 72: loss improved from 1.81262 to 1.73987, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 260ms/step - loss: 1.7399 - lr: 0.0010\n",
            "Epoch 73/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6479\n",
            "Epoch 73: loss improved from 1.73987 to 1.64786, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 263ms/step - loss: 1.6479 - lr: 0.0010\n",
            "Epoch 74/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5910\n",
            "Epoch 74: loss improved from 1.64786 to 1.59095, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 258ms/step - loss: 1.5910 - lr: 0.0010\n",
            "Epoch 75/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5566\n",
            "Epoch 75: loss improved from 1.59095 to 1.55661, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 254ms/step - loss: 1.5566 - lr: 0.0010\n",
            "Epoch 76/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5388\n",
            "Epoch 76: loss improved from 1.55661 to 1.53877, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 250ms/step - loss: 1.5388 - lr: 0.0010\n",
            "Epoch 77/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5333\n",
            "Epoch 77: loss improved from 1.53877 to 1.53329, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 275ms/step - loss: 1.5333 - lr: 0.0010\n",
            "Epoch 78/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4961\n",
            "Epoch 78: loss improved from 1.53329 to 1.49608, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 251ms/step - loss: 1.4961 - lr: 0.0010\n",
            "Epoch 79/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4790\n",
            "Epoch 79: loss improved from 1.49608 to 1.47896, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 257ms/step - loss: 1.4790 - lr: 0.0010\n",
            "Epoch 80/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4432\n",
            "Epoch 80: loss improved from 1.47896 to 1.44322, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 251ms/step - loss: 1.4432 - lr: 0.0010\n",
            "Epoch 81/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4296\n",
            "Epoch 81: loss improved from 1.44322 to 1.42961, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 252ms/step - loss: 1.4296 - lr: 0.0010\n",
            "Epoch 82/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.4009\n",
            "Epoch 82: loss improved from 1.42961 to 1.40092, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 253ms/step - loss: 1.4009 - lr: 0.0010\n",
            "Epoch 83/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3981\n",
            "Epoch 83: loss improved from 1.40092 to 1.39811, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 269ms/step - loss: 1.3981 - lr: 0.0010\n",
            "Epoch 84/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3558\n",
            "Epoch 84: loss improved from 1.39811 to 1.35576, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 255ms/step - loss: 1.3558 - lr: 0.0010\n",
            "Epoch 85/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3364\n",
            "Epoch 85: loss improved from 1.35576 to 1.33641, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 249ms/step - loss: 1.3364 - lr: 0.0010\n",
            "Epoch 86/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3309\n",
            "Epoch 86: loss improved from 1.33641 to 1.33094, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 255ms/step - loss: 1.3309 - lr: 0.0010\n",
            "Epoch 87/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.3162\n",
            "Epoch 87: loss improved from 1.33094 to 1.31620, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 285ms/step - loss: 1.3162 - lr: 0.0010\n",
            "Epoch 88/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2946\n",
            "Epoch 88: loss improved from 1.31620 to 1.29462, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 249ms/step - loss: 1.2946 - lr: 0.0010\n",
            "Epoch 89/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2740\n",
            "Epoch 89: loss improved from 1.29462 to 1.27404, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 251ms/step - loss: 1.2740 - lr: 0.0010\n",
            "Epoch 90/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2627\n",
            "Epoch 90: loss improved from 1.27404 to 1.26266, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 285ms/step - loss: 1.2627 - lr: 0.0010\n",
            "Epoch 91/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2423\n",
            "Epoch 91: loss improved from 1.26266 to 1.24229, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 270ms/step - loss: 1.2423 - lr: 0.0010\n",
            "Epoch 92/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2178\n",
            "Epoch 92: loss improved from 1.24229 to 1.21782, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 252ms/step - loss: 1.2178 - lr: 0.0010\n",
            "Epoch 93/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2087\n",
            "Epoch 93: loss improved from 1.21782 to 1.20867, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 265ms/step - loss: 1.2087 - lr: 0.0010\n",
            "Epoch 94/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.2033\n",
            "Epoch 94: loss improved from 1.20867 to 1.20329, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 260ms/step - loss: 1.2033 - lr: 0.0010\n",
            "Epoch 95/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1856\n",
            "Epoch 95: loss improved from 1.20329 to 1.18563, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 254ms/step - loss: 1.1856 - lr: 0.0010\n",
            "Epoch 96/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1891\n",
            "Epoch 96: loss did not improve from 1.18563\n",
            "61/61 [==============================] - 15s 249ms/step - loss: 1.1891 - lr: 0.0010\n",
            "Epoch 97/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1929\n",
            "Epoch 97: loss did not improve from 1.18563\n",
            "61/61 [==============================] - 15s 246ms/step - loss: 1.1929 - lr: 0.0010\n",
            "Epoch 98/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1752\n",
            "Epoch 98: loss improved from 1.18563 to 1.17518, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 1.1752 - lr: 0.0010\n",
            "Epoch 99/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1634\n",
            "Epoch 99: loss improved from 1.17518 to 1.16343, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 242ms/step - loss: 1.1634 - lr: 0.0010\n",
            "Epoch 100/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1625\n",
            "Epoch 100: loss improved from 1.16343 to 1.16254, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 274ms/step - loss: 1.1625 - lr: 0.0010\n",
            "Epoch 101/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1540\n",
            "Epoch 101: loss improved from 1.16254 to 1.15403, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 261ms/step - loss: 1.1540 - lr: 0.0010\n",
            "Epoch 102/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1464\n",
            "Epoch 102: loss improved from 1.15403 to 1.14640, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 250ms/step - loss: 1.1464 - lr: 0.0010\n",
            "Epoch 103/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1722\n",
            "Epoch 103: loss did not improve from 1.14640\n",
            "61/61 [==============================] - 16s 263ms/step - loss: 1.1722 - lr: 0.0010\n",
            "Epoch 104/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1910\n",
            "Epoch 104: loss did not improve from 1.14640\n",
            "61/61 [==============================] - 15s 238ms/step - loss: 1.1910 - lr: 0.0010\n",
            "Epoch 105/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1606\n",
            "Epoch 105: loss did not improve from 1.14640\n",
            "\n",
            "Epoch 105: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "61/61 [==============================] - 15s 253ms/step - loss: 1.1606 - lr: 0.0010\n",
            "Epoch 106/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8836\n",
            "Epoch 106: loss improved from 1.14640 to 0.88362, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 273ms/step - loss: 0.8836 - lr: 2.0000e-04\n",
            "Epoch 107/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7756\n",
            "Epoch 107: loss improved from 0.88362 to 0.77557, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 0.7756 - lr: 2.0000e-04\n",
            "Epoch 108/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7414\n",
            "Epoch 108: loss improved from 0.77557 to 0.74136, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 280ms/step - loss: 0.7414 - lr: 2.0000e-04\n",
            "Epoch 109/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7261\n",
            "Epoch 109: loss improved from 0.74136 to 0.72610, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 259ms/step - loss: 0.7261 - lr: 2.0000e-04\n",
            "Epoch 110/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7181\n",
            "Epoch 110: loss improved from 0.72610 to 0.71815, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 253ms/step - loss: 0.7181 - lr: 2.0000e-04\n",
            "Epoch 111/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7112\n",
            "Epoch 111: loss improved from 0.71815 to 0.71117, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 257ms/step - loss: 0.7112 - lr: 2.0000e-04\n",
            "Epoch 112/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7081\n",
            "Epoch 112: loss improved from 0.71117 to 0.70813, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 258ms/step - loss: 0.7081 - lr: 2.0000e-04\n",
            "Epoch 113/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7064\n",
            "Epoch 113: loss improved from 0.70813 to 0.70645, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 256ms/step - loss: 0.7064 - lr: 2.0000e-04\n",
            "Epoch 114/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7044\n",
            "Epoch 114: loss improved from 0.70645 to 0.70443, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 252ms/step - loss: 0.7044 - lr: 2.0000e-04\n",
            "Epoch 115/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7016\n",
            "Epoch 115: loss improved from 0.70443 to 0.70159, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 256ms/step - loss: 0.7016 - lr: 2.0000e-04\n",
            "Epoch 116/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7004\n",
            "Epoch 116: loss improved from 0.70159 to 0.70044, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 254ms/step - loss: 0.7004 - lr: 2.0000e-04\n",
            "Epoch 117/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6998\n",
            "Epoch 117: loss improved from 0.70044 to 0.69979, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 255ms/step - loss: 0.6998 - lr: 2.0000e-04\n",
            "Epoch 118/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6969\n",
            "Epoch 118: loss improved from 0.69979 to 0.69687, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 283ms/step - loss: 0.6969 - lr: 2.0000e-04\n",
            "Epoch 119/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6989\n",
            "Epoch 119: loss did not improve from 0.69687\n",
            "61/61 [==============================] - 15s 251ms/step - loss: 0.6989 - lr: 2.0000e-04\n",
            "Epoch 120/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6947\n",
            "Epoch 120: loss improved from 0.69687 to 0.69474, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 261ms/step - loss: 0.6947 - lr: 2.0000e-04\n",
            "Epoch 121/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6957\n",
            "Epoch 121: loss did not improve from 0.69474\n",
            "61/61 [==============================] - 16s 256ms/step - loss: 0.6957 - lr: 2.0000e-04\n",
            "Epoch 122/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6934\n",
            "Epoch 122: loss improved from 0.69474 to 0.69340, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 257ms/step - loss: 0.6934 - lr: 2.0000e-04\n",
            "Epoch 123/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6925\n",
            "Epoch 123: loss improved from 0.69340 to 0.69252, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 275ms/step - loss: 0.6925 - lr: 2.0000e-04\n",
            "Epoch 124/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6937\n",
            "Epoch 124: loss did not improve from 0.69252\n",
            "61/61 [==============================] - 15s 251ms/step - loss: 0.6937 - lr: 2.0000e-04\n",
            "Epoch 125/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6949\n",
            "Epoch 125: loss did not improve from 0.69252\n",
            "61/61 [==============================] - 15s 248ms/step - loss: 0.6949 - lr: 2.0000e-04\n",
            "Epoch 126/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6914\n",
            "Epoch 126: loss improved from 0.69252 to 0.69141, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 245ms/step - loss: 0.6914 - lr: 2.0000e-04\n",
            "Epoch 127/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6907\n",
            "Epoch 127: loss improved from 0.69141 to 0.69065, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 271ms/step - loss: 0.6907 - lr: 2.0000e-04\n",
            "Epoch 128/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6925\n",
            "Epoch 128: loss did not improve from 0.69065\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 0.6925 - lr: 2.0000e-04\n",
            "Epoch 129/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6917\n",
            "Epoch 129: loss did not improve from 0.69065\n",
            "61/61 [==============================] - 18s 299ms/step - loss: 0.6917 - lr: 2.0000e-04\n",
            "Epoch 130/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6899\n",
            "Epoch 130: loss improved from 0.69065 to 0.68989, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 0.6899 - lr: 2.0000e-04\n",
            "Epoch 131/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6898\n",
            "Epoch 131: loss improved from 0.68989 to 0.68978, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 262ms/step - loss: 0.6898 - lr: 2.0000e-04\n",
            "Epoch 132/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6879\n",
            "Epoch 132: loss improved from 0.68978 to 0.68794, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 266ms/step - loss: 0.6879 - lr: 2.0000e-04\n",
            "Epoch 133/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6877\n",
            "Epoch 133: loss improved from 0.68794 to 0.68768, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 268ms/step - loss: 0.6877 - lr: 2.0000e-04\n",
            "Epoch 134/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6868\n",
            "Epoch 134: loss improved from 0.68768 to 0.68682, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 253ms/step - loss: 0.6868 - lr: 2.0000e-04\n",
            "Epoch 135/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6862\n",
            "Epoch 135: loss improved from 0.68682 to 0.68624, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 0.6862 - lr: 2.0000e-04\n",
            "Epoch 136/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6881\n",
            "Epoch 136: loss did not improve from 0.68624\n",
            "61/61 [==============================] - 14s 231ms/step - loss: 0.6881 - lr: 2.0000e-04\n",
            "Epoch 137/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6872\n",
            "Epoch 137: loss did not improve from 0.68624\n",
            "61/61 [==============================] - 14s 233ms/step - loss: 0.6872 - lr: 2.0000e-04\n",
            "Epoch 138/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6867\n",
            "Epoch 138: loss did not improve from 0.68624\n",
            "\n",
            "Epoch 138: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "61/61 [==============================] - 16s 257ms/step - loss: 0.6867 - lr: 2.0000e-04\n",
            "Epoch 139/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6416\n",
            "Epoch 139: loss improved from 0.68624 to 0.64158, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 243ms/step - loss: 0.6416 - lr: 1.0000e-04\n",
            "Epoch 140/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6395\n",
            "Epoch 140: loss improved from 0.64158 to 0.63952, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 253ms/step - loss: 0.6395 - lr: 1.0000e-04\n",
            "Epoch 141/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6391\n",
            "Epoch 141: loss improved from 0.63952 to 0.63911, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 0.6391 - lr: 1.0000e-04\n",
            "Epoch 142/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6390\n",
            "Epoch 142: loss improved from 0.63911 to 0.63898, saving model to nextword1.h5\n",
            "61/61 [==============================] - 16s 255ms/step - loss: 0.6390 - lr: 1.0000e-04\n",
            "Epoch 143/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6385\n",
            "Epoch 143: loss improved from 0.63898 to 0.63850, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 251ms/step - loss: 0.6385 - lr: 1.0000e-04\n",
            "Epoch 144/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6383\n",
            "Epoch 144: loss improved from 0.63850 to 0.63830, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 244ms/step - loss: 0.6383 - lr: 1.0000e-04\n",
            "Epoch 145/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6374\n",
            "Epoch 145: loss improved from 0.63830 to 0.63742, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 243ms/step - loss: 0.6374 - lr: 1.0000e-04\n",
            "Epoch 146/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6378\n",
            "Epoch 146: loss did not improve from 0.63742\n",
            "61/61 [==============================] - 14s 237ms/step - loss: 0.6378 - lr: 1.0000e-04\n",
            "Epoch 147/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6368\n",
            "Epoch 147: loss improved from 0.63742 to 0.63678, saving model to nextword1.h5\n",
            "61/61 [==============================] - 15s 242ms/step - loss: 0.6368 - lr: 1.0000e-04\n",
            "Epoch 148/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6369\n",
            "Epoch 148: loss did not improve from 0.63678\n",
            "61/61 [==============================] - 14s 232ms/step - loss: 0.6369 - lr: 1.0000e-04\n",
            "Epoch 149/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6369\n",
            "Epoch 149: loss did not improve from 0.63678\n",
            "61/61 [==============================] - 15s 250ms/step - loss: 0.6369 - lr: 1.0000e-04\n",
            "Epoch 150/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6377\n",
            "Epoch 150: loss did not improve from 0.63678\n",
            "61/61 [==============================] - 15s 248ms/step - loss: 0.6377 - lr: 1.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f46a4397710>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Libraries\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "\n",
        "model = load_model('nextword1.h5')\n",
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "        In this function we are using the tokenizer and models trained\n",
        "        and we are creating the sequence of the text entered and then\n",
        "        using our model to predict and return the the predicted word.\n",
        "    \n",
        "    \"\"\"\n",
        "    for i in range(3):\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = np.array(sequence)\n",
        "        \n",
        "        preds = model.predict_classes(sequence)\n",
        "#         print(preds)\n",
        "        predicted_word = \"\"\n",
        "        \n",
        "        for key, value in tokenizer.word_index.items():\n",
        "            if value == preds:\n",
        "                predicted_word = key\n",
        "                break\n",
        "        \n",
        "        print(predicted_word)\n",
        "        return predicted_word"
      ],
      "metadata": {
        "id": "8WlzwGYhzKsh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    We are testing our model and we will run the model\n",
        "    until the user decides to stop the script.\n",
        "    While the script is running we try and check if \n",
        "    the prediction can be made on the text. If no\n",
        "    prediction can be made we just continue.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# text1 = \"at the dull\"\n",
        "# text2 = \"collection of textile\"\n",
        "# text3 = \"what a strenuous\"\n",
        "# text4 = \"stop the script\"\n",
        "\n",
        "while(True):\n",
        "\n",
        "    text = input(\"Enter your line: \")\n",
        "    \n",
        "    if text == \"stop the script\":\n",
        "        print(\"Ending The Program.....\")\n",
        "        break\n",
        "    \n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(\" \")\n",
        "            text = text[-1]\n",
        "\n",
        "            text = ''.join(text)\n",
        "            Predict_Next_Words(model, tokenizer, text)\n",
        "            \n",
        "        except:\n",
        "            continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4KYHx7G_csT",
        "outputId": "733b5518-e783-411b-c354-c4ca2382ef82"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your line: at the dull\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tx0iTs1u_6iu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZIPgzZOoP8TrbZQeJ3PYn"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}